{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del algoritmo Apriori\n",
    "\n",
    "##### Por: Daniela Flores Villanueva\n",
    "\n",
    "En este *notebook* se define lo necesario para implementar el algoritmo Apriori con la menor dependencia de código externo posible. Posteriormente, se emplea la implementación propuesta para minar reglas de asociación que presenten altos valores para ciertas métricas y se analizan separadamente. Finalmente, se entrega una forma de visualizar un conjunto de reglas de asociación.\n",
    "\n",
    "## Librerías utilizadas\n",
    "\n",
    "- `numpy`: se utilizó para cargar los datos del Spotify RecSys Challenge, que venían en una arreglo serializado de `numpy`.\n",
    "- `collections`: se usó su clase `defaultdict`, para formar los diccionarios con los contadores de soporte de cada *itemset*.\n",
    "- `itertools`: se empleó su método `combinations`, para generar todos los subconjuntos de largo `k` que pudiesen extraerse de un conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T16:19:05.294873Z",
     "start_time": "2018-09-21T16:19:04.233557Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar todas las combinaciones largo $r$ que pueden extraerse de un iterable, se define la siguiente función. Esta implementación fue extraída de [este](https://docs.python.org/3/library/itertools.html#itertools.combinations) recurso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T16:19:05.312937Z",
     "start_time": "2018-09-21T16:19:05.302154Z"
    }
   },
   "outputs": [],
   "source": [
    "def combinations(iterable, r):\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    if r > n:\n",
    "        return\n",
    "    indices = list(range(r))\n",
    "    yield tuple(pool[i] for i in indices)\n",
    "    while True:\n",
    "        for i in reversed(range(r)):\n",
    "            if indices[i] != i + n - r:\n",
    "                break\n",
    "        else:\n",
    "            return\n",
    "        indices[i] += 1\n",
    "        for j in range(i + 1, r):\n",
    "            indices[j] = indices[j - 1] + 1\n",
    "        yield tuple(pool[i] for i in indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase Apriori\n",
    "\n",
    "La implementación presentada en este *notebook* se basa en la creación de un objeto de clase `Apriori` que, en primer lugar, recibe los siguientes parámetros:\n",
    "\n",
    "- `data`: corresponde a las transacciones, en forma de `numpy.array`.\n",
    "- `min_support`: soporte mínimo para que un `itemset` sea considerado frecuente.\n",
    "- `min_confidence`: confianza mínima para que cierta regla de asociación sea retornada.\n",
    "- `min_lift`: *lift* mínimo para que cierta regla de asociación sea retornada.\n",
    "\n",
    "### Métodos\n",
    "\n",
    "- `prepare_data`: convierte los datos recibidos (`self.data`) en una lista de conjuntos, donde cada conjunto corresponde a una lista de reproducción. De esta forma, se evitan canciones repetidas en las listas. También, se guarda en `self.songs_counter` un contador con la frecuencia de cada canción. Esta información permitirá formar el conjunto L_1.\n",
    "- `get_songs_appearances`: gracias a este método, se guarda un diccionario cuyas llaves son nombres de canciones y los valores el conjunto de índices de las listas de reproducción donde aparece dicha canción. Esto permitirá obtener el contador de soporte de un *itemset* con facilidad.\n",
    "- `generate_L_1`: con esto se obtienen los *itemsets* frecuentes de largo 1, es decir, las canciones que aparecen un porcentaje de veces mayor al mínimo soporte definido.\n",
    "- `has_unfrequent_subset`: este método permite verificar la propiedad de monotonicidad de un *k-itemset*: si el itemset es frecuente, entonces todos sus subconjuntos deben serlo también. Este método finalmente no se ocupó, pues demoraba la ejecución del método `fit`. Cabe destacar, eso sí, que la poda (realizada en `prune_itemsets`) es lo suficientemente veloz como para prescindir de este paso al menos en esta implementación. Lo correcto hubiese sido, por supuesto, incluir este paso de verificación al generar candidatos a *itemsets* frecuentes.\n",
    "- `generate_new_candidates`: este método permite generar un nuevo conjunto de posibles *itemsets* frecuentes. Esto se hace a partir de la unión de un conjunto de *itemsets* frecuentes consigo mismo. Un *itemset* se \"fusiona\" con otro solo si sus primeros $k - 2$ elementos son iguales. Puede observarse que se dejó comentada la verificación de la propiedad de monotonocidad (`if not self.has_unfrequent_subset(c, current_itemsets, k)`), puesto que su utilización, aunque correcta, agrega alrededor de 8 segundos extra al tiempo de ejecución del método `fit`.\n",
    "- `calculate_subset_count`: calcula el contador de soporte de un *itemset*. Gracias al diccionario generado con `get_songs_appearances`, es fácil obtener el contador de soporte de un *itemset*: basta con obtener las listas de reproducción en que aparece cada canción del *itemset* y luego intersectar esos conjuntos de *playlists*. El largo de la intersección corresponderá al contador de soporte de *itemset*.\n",
    "- `prune_itemsets`: retorna el conjunto de *itemsets* que superan el mínimo soporte definido.\n",
    "- `export_frequent_itemsets_to_csv`: permite exportar los *itemsets* frecuentes junto a su contador de soporte y a su soporte a un archivo `csv`.\n",
    "- `fit`: método que genera los *itemsets* frecuentes.\n",
    "- `get_association_rule_from_itemset`: Dado un *itemset* $I$ se forman todas las combinaciones posibles de $X \\rightarrow Y$, donde $X \\subset I$ y $Y = I - X$. Es importante notar que solo se consideran reglas de asociación aquellas en que tanto X como Y tengan largo mayor o igual a 1.\n",
    "- `generate`: se generan todas las reglas de asociación posibles a partir de los *itemsets* frecuentes encontrados. Por defecto, se retornan ordenadas por *confidence*. El usuario puede cambiar esto mediante la modificación del parámetro `order_by`.\n",
    "- `export_rules_to_csv`: se exportan las reglas a `csv`, gracias a un `DataFrame` de `pandas`.\n",
    "- `get_n_top_rules`: permite obtener las $n$ reglas con mayor valor de la métrica determinada por el usuario (*confidence*, por defecto). Esta métrica puede corresponder a *confidence* o a *lift*. El criterio de ordenación se cambia a través de la modificación del parámetro `order_by`.\n",
    "- `filter_rules`: método que permite filtrar las reglas según los umbrales de *confidence* y *lift* definidos al inicializar el objeto. Se puede filtrar por *confidence* y *lift* tanto de forma separada como integrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T16:19:05.358382Z",
     "start_time": "2018-09-21T16:19:05.317015Z"
    }
   },
   "outputs": [],
   "source": [
    "class Apriori:\n",
    "    def __init__(self, data, min_support=0.01, min_confidence=0.01,\n",
    "                 min_lift=1):\n",
    "        self.data = data\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "        self.min_lift = min_lift\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.playlists = list(self.data.item().values())\n",
    "        self.playlists = [set(playlist) for playlist in self.playlists]\n",
    "        unique_songs = [item for sublist in self.playlists for item in sublist]\n",
    "        self.songs_counter = pd.Series(\n",
    "            data=unique_songs).value_counts().to_dict()\n",
    "\n",
    "    def get_songs_appearances(self):\n",
    "        songs_in_playlists = collections.defaultdict(set)\n",
    "        for index, playlist in enumerate(self.playlists):\n",
    "            for song in playlist:\n",
    "                songs_in_playlists[song].add(index)\n",
    "        self.songs_in_playlists = songs_in_playlists\n",
    "\n",
    "    def generate_L_1(self):\n",
    "        self.L_1_counter = {\n",
    "            song: times\n",
    "            for song, times in self.songs_counter.items()\n",
    "            if times / len(self.playlists) >= self.min_support\n",
    "        }\n",
    "        self.L_1 = [{song} for song in self.L_1_counter.keys()]\n",
    "\n",
    "    def has_unfrequent_subset(self, candidate, current_itemsets, k):\n",
    "        subsets = combinations(candidate, k - 1)\n",
    "        for subset in subsets:\n",
    "            if frozenset(subset) not in current_itemsets:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def generate_new_candidates(self, current_itemsets, k):\n",
    "        C_k = set()\n",
    "        m = k - 2\n",
    "        for candidate in current_itemsets:\n",
    "            candidate = list(candidate)\n",
    "            for aux_candidate in current_itemsets:\n",
    "                aux_candidate = list(aux_candidate)\n",
    "                can_join = True\n",
    "                for i in range(k - 2):\n",
    "                    if candidate[i] != aux_candidate[i]:\n",
    "                        can_join = False\n",
    "                        break\n",
    "                if not can_join:\n",
    "                    continue\n",
    "                if candidate[k - 2] < aux_candidate[k - 2]:\n",
    "                    c = candidate + [aux_candidate[k - 2]]\n",
    "                    c = frozenset(sorted(c))\n",
    "                    #                     if not self.has_unfrequent_subset(c, current_itemsets, k):\n",
    "                    #                         C_k.add(c)\n",
    "                    C_k.add(c)\n",
    "        return C_k\n",
    "\n",
    "    def calculate_subset_count(self, subset):\n",
    "        playlists_inter = []\n",
    "        for song in subset:\n",
    "            playlists_inter.append(self.songs_in_playlists[song])\n",
    "        return len(set.intersection(*playlists_inter))\n",
    "\n",
    "    def prune_itemsets(self, C_k):\n",
    "        C_k_counter = {}\n",
    "        playlist_length = len(self.playlists)\n",
    "        for candidate in C_k:\n",
    "            C_k_counter[candidate] = self.calculate_subset_count(candidate)\n",
    "        L_k_counter = {\n",
    "            subset: times\n",
    "            for subset, times in C_k_counter.items()\n",
    "            if times / playlist_length >= self.min_support\n",
    "        }\n",
    "        return L_k_counter\n",
    "\n",
    "    def export_frequent_itemsets_to_csv(self):\n",
    "        self.frequent_itemsets_df = pd.DataFrame([\n",
    "            item for sublist in self.k_frequent_itemsets.values()\n",
    "            for item in sublist\n",
    "        ]).round(3)\n",
    "        self.frequent_itemsets_df.columns = [\"itemset\", \"count_support\"]\n",
    "        self.frequent_itemsets_df[\"support\"] = self.frequent_itemsets_df[\n",
    "            \"count_support\"] / len(self.playlists)\n",
    "        print(\"Generando archivo csv con itemsets frecuentes.\")\n",
    "        self.frequent_itemsets_df.to_csv(\n",
    "            \"frequent_itemsets.csv\", index=\"False\")\n",
    "        return self.frequent_itemsets_df\n",
    "\n",
    "    def fit(self):\n",
    "        self.prepare_data()\n",
    "        self.get_songs_appearances()\n",
    "        self.generate_L_1()\n",
    "        self.k_frequent_itemsets = {}\n",
    "        self.k_frequent_itemsets[1] = sorted(\n",
    "            self.L_1_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.frequent_itemsets_length_gt2 = []\n",
    "        k = 2\n",
    "        current = self.L_1\n",
    "        while len(current) != 0:\n",
    "            C_k = self.generate_new_candidates(current, k)\n",
    "            L_k_counter = self.prune_itemsets(C_k)\n",
    "            L_k = L_k_counter.keys()\n",
    "            self.frequent_itemsets_length_gt2.extend(L_k)\n",
    "            self.k_frequent_itemsets[k] = sorted(\n",
    "                L_k_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "            k += 1\n",
    "            current = L_k\n",
    "        self.export_frequent_itemsets_to_csv()\n",
    "\n",
    "    def get_association_rule_from_itemset(self, itemset):\n",
    "        itemset_count = self.calculate_subset_count(itemset)\n",
    "        itemset_support = itemset_count / len(self.playlists)\n",
    "        for i in range(1, len(itemset) + 1):\n",
    "            for x_set in combinations(itemset, i):\n",
    "                x_set = set(x_set)\n",
    "                y_set = set(itemset) - x_set\n",
    "                x_support = self.calculate_subset_count(x_set) / len(\n",
    "                    self.playlists)\n",
    "                x_y_support = self.calculate_subset_count(\n",
    "                    x_set.union(y_set)) / len(self.playlists)\n",
    "                rule_confidence = x_y_support / x_support\n",
    "                if len(x_set) > 0 and len(y_set) > 0:\n",
    "                    y_support = self.calculate_subset_count(y_set) / len(\n",
    "                        self.playlists)\n",
    "                    rule_lift = x_y_support / (x_support * y_support)\n",
    "                    self.rules.append((x_set, y_set, rule_confidence,\n",
    "                                       rule_lift))\n",
    "\n",
    "    def generate(self):\n",
    "        self.rules = []\n",
    "        for itemset in self.frequent_itemsets_length_gt2:\n",
    "            self.get_association_rule_from_itemset(itemset)\n",
    "        self.export_rules_to_csv()\n",
    "\n",
    "    def export_rules_to_csv(self):\n",
    "        self.rules_df = pd.DataFrame(\n",
    "            data=self.rules,\n",
    "            columns=[\"antecedent\", \"consequent\", \"confidence\",\n",
    "                     \"lift\"]).round(3)\n",
    "        print(\"Generando archivo csv con las reglas de asociación.\")\n",
    "        self.rules_df.to_csv(\"association_rules.csv\", index=False)\n",
    "        return self.rules_df\n",
    "\n",
    "    def get_n_top_rules(self, n, order_by=\"confidence\"):\n",
    "        sorted_df = self.rules_df.sort_values(\n",
    "            order_by, ascending=False).head(n)\n",
    "        print(\"Generando archivo csv con top {} reglas ordenadas según {}.\".\n",
    "              format(n, order_by))\n",
    "        sorted_df.to_csv(\n",
    "            \"top_{}_rule_by_{}.csv\".format(n, order_by), index=False)\n",
    "        return sorted_df\n",
    "\n",
    "    def filter_rules(self, filter_by=\"both\"):\n",
    "        if filter_by == \"both\":\n",
    "            filtered_rules = self.rules_df[\n",
    "                (self.rules_df[\"confidence\"] >=\n",
    "                self.min_confidence) & (self.rules_df[\"lift\"] >= self.min_lift)]\n",
    "        elif filter_by == \"confidence\":\n",
    "            filtered_rules = self.rules_df[self.rules_df[\"confidence\"] >= self.\n",
    "                                           min_confidence].sort_values(\n",
    "                                               filter_by, ascending=False)\n",
    "        else:\n",
    "            filtered_rules = self.rules_df[\n",
    "                self.rules_df[\"lift\"] >= self.min_lift].sort_values(\n",
    "                    filter_by, ascending=False)\n",
    "        print(\"Generando archivo csv con reglas filtradas. Se imprimirán las 10 primeras reglas.\")\n",
    "        display(filtered_rules.head(10))\n",
    "        filtered_rules.to_csv(\"filtered_rules.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de la implementación sobre la base de datos entregada\n",
    "\n",
    "En primer lugar, se cargan los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T16:19:05.808385Z",
     "start_time": "2018-09-21T16:19:05.371082Z"
    }
   },
   "outputs": [],
   "source": [
    "spotify_data = np.load(\"spotify.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para definir un mínimo soporte adecuado, conviene estudiar la frecuencia de aparición de las distintas canciones del *data set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T16:19:06.502261Z",
     "start_time": "2018-09-21T16:19:05.819972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Closer                         761\n",
       "Home                           489\n",
       "HUMBLE.                        470\n",
       "Roses                          436\n",
       "One Dance                      433\n",
       "Ride                           429\n",
       "Congratulations                410\n",
       "Let Me Love You                403\n",
       "Broccoli (feat. Lil Yachty)    402\n",
       "Caroline                       395\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlists = spotify_data.item().values()\n",
    "unique_songs = [item for sublist in playlists for item in sublist]\n",
    "pd.Series(unique_songs).value_counts().sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible notar que la canción que más veces aparece, lo hace tan solo 761, lo que equivale al 7%. Por esta razón, el mínimo soporte empleado será 0.01. Así, se define un objeto de clase `Apriori`, al que se le entregan los datos (guardados en la variable `spotify_data`), el soporte mínimo deseado, la confianza mínima deseada y el mínimo lift requerido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T16:19:06.512543Z",
     "start_time": "2018-09-21T16:19:06.505548Z"
    }
   },
   "outputs": [],
   "source": [
    "apriori = Apriori(\n",
    "    data=spotify_data, min_support=0.01, min_confidence=0.5, min_lift=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el método *fit*, se generan los *itemsets* frecuentes, que permitirán obtener las reglas de asociación. Los *itemsets* se guardan en un archivo `.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T16:19:12.364575Z",
     "start_time": "2018-09-21T16:19:06.516320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando archivo csv con itemsets frecuentes.\n"
     ]
    }
   ],
   "source": [
    "apriori.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los *itemsets* frecuentes ya encontrados, se procede a generar las reglas de asociación. Para no llenar el *notebook* con estas reglas (son 448), se guardan en un archivo `.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T16:19:12.430907Z",
     "start_time": "2018-09-21T16:19:12.368051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando archivo csv con las reglas de asociación.\n"
     ]
    }
   ],
   "source": [
    "apriori.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T15:52:22.682801Z",
     "start_time": "2018-09-21T15:52:22.672716Z"
    }
   },
   "source": [
    "A continuación, se puede obtener las reglas ordenadas según un criterio definido por el usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T16:19:12.516597Z",
     "start_time": "2018-09-21T16:19:12.446050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando archivo csv con top 10 reglas ordenadas según confidence.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedent</th>\n",
       "      <th>consequent</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>{DNA., Mask Off}</td>\n",
       "      <td>{HUMBLE.}</td>\n",
       "      <td>0.909</td>\n",
       "      <td>19.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>{DNA., XO TOUR Llif3}</td>\n",
       "      <td>{HUMBLE.}</td>\n",
       "      <td>0.864</td>\n",
       "      <td>18.589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>{DNA.}</td>\n",
       "      <td>{HUMBLE.}</td>\n",
       "      <td>0.823</td>\n",
       "      <td>17.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>{Mask Off, XO TOUR Llif3}</td>\n",
       "      <td>{HUMBLE.}</td>\n",
       "      <td>0.804</td>\n",
       "      <td>17.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>{Broccoli (feat. Lil Yachty), Bounce Back}</td>\n",
       "      <td>{Bad and Boujee (feat. Lil Uzi Vert)}</td>\n",
       "      <td>0.775</td>\n",
       "      <td>22.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>{Slippery (feat. Gucci Mane), XO TOUR Llif3}</td>\n",
       "      <td>{HUMBLE.}</td>\n",
       "      <td>0.765</td>\n",
       "      <td>16.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>{Tunnel Vision, XO TOUR Llif3}</td>\n",
       "      <td>{HUMBLE.}</td>\n",
       "      <td>0.750</td>\n",
       "      <td>16.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>{Congratulations, Mask Off}</td>\n",
       "      <td>{HUMBLE.}</td>\n",
       "      <td>0.747</td>\n",
       "      <td>16.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>{Bounce Back, Mask Off}</td>\n",
       "      <td>{HUMBLE.}</td>\n",
       "      <td>0.743</td>\n",
       "      <td>15.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>{Mask Off, goosebumps}</td>\n",
       "      <td>{HUMBLE.}</td>\n",
       "      <td>0.743</td>\n",
       "      <td>15.984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       antecedent  \\\n",
       "422                              {DNA., Mask Off}   \n",
       "404                         {DNA., XO TOUR Llif3}   \n",
       "36                                         {DNA.}   \n",
       "399                     {Mask Off, XO TOUR Llif3}   \n",
       "386    {Broccoli (feat. Lil Yachty), Bounce Back}   \n",
       "368  {Slippery (feat. Gucci Mane), XO TOUR Llif3}   \n",
       "417                {Tunnel Vision, XO TOUR Llif3}   \n",
       "374                   {Congratulations, Mask Off}   \n",
       "351                       {Bounce Back, Mask Off}   \n",
       "356                        {Mask Off, goosebumps}   \n",
       "\n",
       "                                consequent  confidence    lift  \n",
       "422                              {HUMBLE.}       0.909  19.550  \n",
       "404                              {HUMBLE.}       0.864  18.589  \n",
       "36                               {HUMBLE.}       0.823  17.688  \n",
       "399                              {HUMBLE.}       0.804  17.283  \n",
       "386  {Bad and Boujee (feat. Lil Uzi Vert)}       0.775  22.469  \n",
       "368                              {HUMBLE.}       0.765  16.455  \n",
       "417                              {HUMBLE.}       0.750  16.129  \n",
       "374                              {HUMBLE.}       0.747  16.063  \n",
       "351                              {HUMBLE.}       0.743  15.971  \n",
       "356                              {HUMBLE.}       0.743  15.984  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apriori.get_n_top_rules(10, \"confidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T16:19:12.584629Z",
     "start_time": "2018-09-21T16:19:12.520873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando archivo csv con reglas filtradas. Se imprimirán las 10 primeras reglas.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedent</th>\n",
       "      <th>consequent</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{Bounce Back}</td>\n",
       "      <td>{Bad and Boujee (feat. Lil Uzi Vert)}</td>\n",
       "      <td>0.569</td>\n",
       "      <td>16.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{Butterfly Effect}</td>\n",
       "      <td>{HUMBLE.}</td>\n",
       "      <td>0.521</td>\n",
       "      <td>11.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{Too Good}</td>\n",
       "      <td>{One Dance}</td>\n",
       "      <td>0.653</td>\n",
       "      <td>15.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{Chill Bill}</td>\n",
       "      <td>{Broccoli (feat. Lil Yachty)}</td>\n",
       "      <td>0.531</td>\n",
       "      <td>13.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{You Was Right}</td>\n",
       "      <td>{Money Longer}</td>\n",
       "      <td>0.603</td>\n",
       "      <td>31.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{Money Longer}</td>\n",
       "      <td>{You Was Right}</td>\n",
       "      <td>0.603</td>\n",
       "      <td>31.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{You Was Right}</td>\n",
       "      <td>{Bad and Boujee (feat. Lil Uzi Vert)}</td>\n",
       "      <td>0.541</td>\n",
       "      <td>15.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{Mask Off}</td>\n",
       "      <td>{HUMBLE.}</td>\n",
       "      <td>0.646</td>\n",
       "      <td>13.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{rockstar}</td>\n",
       "      <td>{HUMBLE.}</td>\n",
       "      <td>0.504</td>\n",
       "      <td>10.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{Controlla}</td>\n",
       "      <td>{One Dance}</td>\n",
       "      <td>0.602</td>\n",
       "      <td>14.646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            antecedent                             consequent  confidence  \\\n",
       "1        {Bounce Back}  {Bad and Boujee (feat. Lil Uzi Vert)}       0.569   \n",
       "3   {Butterfly Effect}                              {HUMBLE.}       0.521   \n",
       "6           {Too Good}                            {One Dance}       0.653   \n",
       "10        {Chill Bill}          {Broccoli (feat. Lil Yachty)}       0.531   \n",
       "18     {You Was Right}                         {Money Longer}       0.603   \n",
       "19      {Money Longer}                        {You Was Right}       0.603   \n",
       "23     {You Was Right}  {Bad and Boujee (feat. Lil Uzi Vert)}       0.541   \n",
       "25          {Mask Off}                              {HUMBLE.}       0.646   \n",
       "26          {rockstar}                              {HUMBLE.}       0.504   \n",
       "28         {Controlla}                            {One Dance}       0.602   \n",
       "\n",
       "      lift  \n",
       "1   16.493  \n",
       "3   11.196  \n",
       "6   15.899  \n",
       "10  13.301  \n",
       "18  31.087  \n",
       "19  31.087  \n",
       "23  15.688  \n",
       "25  13.883  \n",
       "26  10.845  \n",
       "28  14.646  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "apriori.filter_rules(\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "notify_time": "30",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1069px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
